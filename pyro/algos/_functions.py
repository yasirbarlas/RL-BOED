"""Utility functions for NumPy-based Reinforcement learning algorithms."""
import numpy as np
import torch

from pyro.sampler.utils import rollout
from pyro._dtypes import EpisodeBatch

import abc

def obtain_evaluation_episodes(policy, env, max_path_length=1000,
                               num_eps=100, n_parallel=1):
    """Sample the policy for num_eps trajectories and return average values.

    Args:
        policy (garage.Policy): Policy to use as the actor when
            gathering samples.
        env (garage.envs.GarageEnv): The environement used to obtain
            trajectories.
        max_path_length (int): Maximum path length. The episode will
            terminate when length of trajectory reaches max_path_length.
        num_eps (int): Number of trajectories.

    Returns:
        TrajectoryBatch: Evaluation trajectories, representing the best
            current performance of the algorithm.

    """
    paths = []
    # Use a finite length rollout for evaluation.

    path = rollout(env,
                   policy,
                   max_path_length=max_path_length,
                   deterministic=True,
                   n_parallel=n_parallel)
    lengths = torch.full((n_parallel,), max_path_length)
    last_observations = path["observations"][max_path_length-1::max_path_length]
    return EpisodeBatch(env_spec=env.spec,
                        episode_infos=dict(),
                        observations=path["observations"],
                        last_observations=last_observations,
                        masks=None,
                        last_masks=None,
                        actions=path["actions"],
                        rewards=path["rewards"],
                        step_types=path["dones"],
                        env_infos=path["env_infos"],
                        agent_infos=path["agent_infos"],
                        lengths=lengths)

"""Interface of RLAlgorithm."""

class RLAlgorithm(abc.ABC):
    """Base class for all the algorithms.

    Note:
        If the field sampler_cls exists, it will be by Trainer.setup to
        initialize a sampler.

    """

    # pylint: disable=too-few-public-methods

    @abc.abstractmethod
    def train(self, trainer):
        """Obtain samplers and start actual training for each epoch.

        Args:
            trainer (Trainer): Trainer is passed to give algorithm
                the access to trainer.step_epochs(), which provides services
                such as snapshotting and sampler control.

        """

class MetaRLAlgorithm(RLAlgorithm, abc.ABC):
    """Base class for Meta-RL Algorithms."""

    @abc.abstractmethod
    def get_exploration_policy(self):
        """Return a policy used before adaptation to a specific task.

        Each time it is retrieved, this policy should only be evaluated in one
        task.

        Returns:
            Policy: The policy used to obtain samples, which are later used for
                meta-RL adaptation.

        """

    @abc.abstractmethod
    def adapt_policy(self, exploration_policy, exploration_episodes):
        """Produce a policy adapted for a task.

        Args:
            exploration_policy (Policy): A policy which was returned from
                get_exploration_policy(), and which generated
                exploration_trajectories by interacting with an environment.
                The caller may not use this object after passing it into this
                method.
            exploration_episodes (EpisodeBatch): Episodes with which to adapt.
                These are generated by exploration_policy while exploring the
                environment.

        Returns:
            Policy: A policy adapted to the task represented by the
                exploration_episodes.

        """